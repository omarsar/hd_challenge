{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n",
    "from sklearn import pipeline, grid_search\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from nltk.stem.porter import *\n",
    "import re\n",
    "import random\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "#stemmer = PorterStemmer()\n",
    "\n",
    "e_strip_punc = re.compile(r\"[^a-zA-z0-9]+\")\n",
    "e_split_words = re.compile(r\"(\\s[a-z]+)([A-Z][a-z]+)\")\n",
    "\n",
    "random.seed(2016)\n",
    "\n",
    "#load datasets\n",
    "df_train = pd.read_csv('./input/train.csv', encoding=\"ISO-8859-1\")\n",
    "df_test = pd.read_csv('./input/test.csv', encoding=\"ISO-8859-1\")\n",
    "df_attr = pd.read_csv('./input/attributes.csv')\n",
    "df_pro_desc = pd.read_csv('./input/product_descriptions.csv')\n",
    "df_brand = df_attr[df_attr.name == \"MFG Brand Name\"][[\"product_uid\", \"value\"]].rename(columns={\"value\": \"brand\"})\n",
    "#df_material = df_attr[df_attr.name == \"Material\"][[\"product_uid\", \"value\"]].rename(columns={\"value\": \"material\"})\n",
    "\n",
    "num_train = df_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def str_stemmer(s):\n",
    "    s = s.replace(\"'\",\"in.\") # character\n",
    "    s = s.replace(\"inches\",\"in.\") # whole word\n",
    "    s = s.replace(\"inch\",\"in.\") # whole word\n",
    "    s = s.replace(\" in \",\"in. \") # no period\n",
    "    s = s.replace(\" in.\",\"in.\") # prefix space\n",
    "\n",
    "    s = s.replace(\"''\",\"ft.\") # character\n",
    "    s = s.replace(\" feet \",\"ft. \") # whole word\n",
    "    s = s.replace(\"feet\",\"ft.\") # whole word\n",
    "    s = s.replace(\"foot\",\"ft.\") # whole word\n",
    "    s = s.replace(\" ft \",\"ft. \") # no period\n",
    "    s = s.replace(\" ft.\",\"ft.\") # prefix space\n",
    "\n",
    "    s = s.replace(\" pounds \",\"lb. \") # character\n",
    "    s = s.replace(\" pound \",\"lb. \") # whole word\n",
    "    s = s.replace(\"pound\",\"lb.\") # whole word\n",
    "    s = s.replace(\" lb \",\"lb. \") # no period\n",
    "    s = s.replace(\" lb.\",\"lb.\") \n",
    "    s = s.replace(\" lbs \",\"lb. \") \n",
    "    s = s.replace(\"lbs.\",\"lb.\") \n",
    "\n",
    "    s = s.replace(\"*\",\" xby \")\n",
    "    s = s.replace(\" by\",\" xby\")\n",
    "    s = s.replace(\"x0\",\" xby 0\")\n",
    "    s = s.replace(\"x1\",\" xby 1\")\n",
    "    s = s.replace(\"x2\",\" xby 2\")\n",
    "    s = s.replace(\"x3\",\" xby 3\")\n",
    "    s = s.replace(\"x4\",\" xby 4\")\n",
    "    s = s.replace(\"x5\",\" xby 5\")\n",
    "    s = s.replace(\"x6\",\" xby 6\")\n",
    "    s = s.replace(\"x7\",\" xby 7\")\n",
    "    s = s.replace(\"x8\",\" xby 8\")\n",
    "    s = s.replace(\"x9\",\" xby 9\")\n",
    "\n",
    "    s = s.replace(\" sq ft\",\"sq.ft. \") \n",
    "    s = s.replace(\"sq ft\",\"sq.ft. \")\n",
    "    s = s.replace(\"sqft\",\"sq.ft. \")\n",
    "    s = s.replace(\" sqft \",\"sq.ft. \") \n",
    "    s = s.replace(\"sq. ft\",\"sq.ft. \") \n",
    "    s = s.replace(\"sq ft.\",\"sq.ft. \") \n",
    "    s = s.replace(\"sq feet\",\"sq.ft. \") \n",
    "    s = s.replace(\"square feet\",\"sq.ft. \") \n",
    "\n",
    "    s = s.replace(\" gallons \",\"gal. \") # character\n",
    "    s = s.replace(\" gallon \",\"gal. \") # whole word\n",
    "    s = s.replace(\"gallons\",\"gal.\") # character\n",
    "    s = s.replace(\"gallon\",\"gal.\") # whole word\n",
    "    s = s.replace(\" gal \",\"gal. \") # character\n",
    "    s = s.replace(\" gal\",\"gal\") # whole word\n",
    "\n",
    "    s = s.replace(\" ounces\",\"oz.\")\n",
    "    s = s.replace(\" ounce\",\"oz.\")\n",
    "    s = s.replace(\"ounce\",\"oz.\")\n",
    "    s = s.replace(\" oz \",\"oz. \")\n",
    "\n",
    "    s = s.replace(\" centimeters\",\"cm.\")    \n",
    "    s = s.replace(\" cm.\",\"cm.\")\n",
    "    s = s.replace(\" cm \",\"cm. \")\n",
    "\n",
    "    s = s.replace(\" milimeters\",\"mm.\")\n",
    "    s = s.replace(\" mm.\",\"mm.\")\n",
    "    s = s.replace(\" mm \",\"mm. \")\n",
    "    return \" \".join([stemmer.stem(word) for word in s.lower().split()])\n",
    "\n",
    "def str_common_word(str1, str2):\n",
    "    return sum(int(str2.find(word)>=0) for word in str1.split())\n",
    "\n",
    "#improved common word matching\n",
    "def str_common_word_2(str1, str2):\n",
    "    words, cnt = str1.split(),0\n",
    "    for word in words:\n",
    "        if len(word)>1:\n",
    "            if str2.find(word)>=0:\n",
    "                cnt+=1\n",
    "    return cnt\n",
    "\n",
    "def str_whole_word(str1, str2, i_):\n",
    "    cnt = 0\n",
    "    while i_ < len(str2):\n",
    "        i_ = str2.find(str1, i_)\n",
    "        if i_ == -1:\n",
    "            return cnt\n",
    "        else:\n",
    "            cnt += 1\n",
    "            i_ += len(str1)\n",
    "    return cnt\n",
    "\n",
    "#calculating n_grams\n",
    "def n_grams_match(x, query, text, n_grams):\n",
    "    q = x[query]\n",
    "    t = x[text]\n",
    "    c = 0\n",
    "    for i in range(len(query) - n_grams + 1):\n",
    "        sq = q[i:i+n_grams]\n",
    "        c += t.count(sq)\n",
    "    return c / (len(text) + len(query))\n",
    "\n",
    "def n_gram_features(df_all):\n",
    "    print(\"Calculating n-gram features\")\n",
    "    for i in range(3, 6):\n",
    "        print(\"Starting n-grams\", i)\n",
    "        #df_all['n_grams_clean_{0}'.format(i)] = df_all.apply(n_grams_match, axis=1, query = 'clean_term', text='clean_brand', n_grams = i)\n",
    "        #df_all['n_grams_stemmed_{0}'.format(i)] = df_all.apply(n_grams_match, axis=1, query = 'search_term', text='brand', n_grams = i)\n",
    "        df_all['n_grams_clean_title_{0}'.format(i)] = df_all.apply(n_grams_match, axis=1, query = 'clean_term', text='clean_title', n_grams = i)\n",
    "        df_all['n_grams_stemmed_title_{0}'.format(i)] = df_all.apply(n_grams_match, axis=1, query = 'search_term', text='product_title', n_grams = i)\n",
    "    print(\"n-grams completed.\")\n",
    "\n",
    "#cleaning the text from unwanted punctuations and unecessary numbers\n",
    "def clean_text(d):\n",
    "    no_punc = e_strip_punc.sub(\" \", d)\n",
    "    words_split = e_split_words.sub(r\"\\1 \\2\", no_punc)\n",
    "    return words_split.lower()\n",
    "\n",
    "def fmean_squared_error(ground_truth, predictions):\n",
    "    fmean_squared_error_ = mean_squared_error(ground_truth, predictions)**0.5\n",
    "    return fmean_squared_error_\n",
    "\n",
    "RMSE = make_scorer(fmean_squared_error, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#merge testing and training datasets together\n",
    "df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n",
    "df_all = pd.merge(df_all, df_pro_desc, how='left', on='product_uid')\n",
    "df_all = pd.merge(df_all, df_brand, how='left', on='product_uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#CLEANING PROCESS \n",
    "df_all['clean_title'] = df_all['product_title'].map(lambda x:clean_text(x))\n",
    "df_all['clean_term'] = df_all['search_term'].map(lambda x:clean_text(x))\n",
    "#df_all['clean_brand'] = df_all['brand'].map(lambda x:clean_text(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#STEMMING PROCESS\n",
    "df_all['search_term'] = df_all['search_term'].map(lambda x:str_stemmer(str(x)))\n",
    "df_all['product_title'] = df_all['product_title'].map(lambda x:str_stemmer(str(x)))\n",
    "df_all['brand'] = df_all['brand'].map(lambda x:str_stemmer(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating n-gram features\n",
      "Starting n-grams 3\n",
      "Starting n-grams 4\n",
      "Starting n-grams 5\n",
      "n-grams completed.\n"
     ]
    }
   ],
   "source": [
    "#NGRAMS CALCULATION\n",
    "n_gram_features(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a pickle file for this feature!\n"
     ]
    }
   ],
   "source": [
    "#IF STEMMER CHANGES NEED TO BE REPRODUCED AGAIN!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "#takes a while to process\n",
    "#switch (False if no pickle exist, True if the pickle exist already)\n",
    "file=True\n",
    "\n",
    "if not file:\n",
    "    print (\"There is >>> NOT << a pickle file for this feature!\")\n",
    "    df_all['product_description'] = df_all['product_description'].map(lambda x:str_stemmer(x))\n",
    "    \n",
    "    #function to write out big features (feature that take long to process)\n",
    "    import pickle as pickle\n",
    "    feature_name = \"product_description\"\n",
    "    pickle_file = feature_name + \".pickle\"\n",
    "    def pickle_feature(feature):\n",
    "        try:\n",
    "            f = open(pickle_file, 'wb')\n",
    "            save = {\n",
    "                feature_name: feature,\n",
    "            }\n",
    "            pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "            f.close()\n",
    "        except Exception as e:\n",
    "            print (\"unable to save data to\", pickle_file, ':', e)\n",
    "            raise\n",
    "    pickle_feature(df_all[feature_name])\n",
    "    \n",
    "else:\n",
    "    file = open('product_description.pickle','rb')\n",
    "    print (\"There is a pickle file for this feature!\")\n",
    "    data = pickle.load(file)\n",
    "    df_all['product_description'] = data['product_description']\n",
    "    del data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#length of query as a feature\n",
    "df_all['len_of_query'] = df_all['search_term'].map(lambda x:len(x.split())).astype(np.int64)\n",
    "df_all['len_of_title'] = df_all['product_title'].map(lambda x:len(x.split())).astype(np.int64)\n",
    "df_all['len_of_brand'] = df_all['brand'].map(lambda x:len(x.split())).astype(np.int64)\n",
    "#df_all['len_of_description'] = df_all['product_description'].map(lambda x:len(x.split())).astype(np.int64)\n",
    "\n",
    "df_all['len_of_query_in_chars'] = df_all['search_term'].map(lambda x:len(x)).astype(np.int64)\n",
    "df_all['len_of_title_in_chars'] = df_all['product_title'].map(lambda x:len(x)).astype(np.int64)\n",
    "df_all['len_of_brand_in_chars'] =  df_all['brand'].map(lambda x:len(x)).astype(np.int64)\n",
    "#df_all['len_of_description_in_chars'] = df_all['product_description'].map(lambda x:len(x)).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#another feature with the combination of all the following columns (search term, product title, description)\n",
    "df_all['product_info'] = df_all['search_term']+\"\\t\"+df_all['product_title']+\"\\t\"+df_all['product_description']+\"\\t\"+df_all['brand']\n",
    "\n",
    "#common words as features\n",
    "df_all['word_in_title'] = df_all['product_info'].map(lambda x:str_common_word_2(x.split('\\t')[0],x.split('\\t')[1]))\n",
    "df_all['word_in_description'] = df_all['product_info'].map(lambda x:str_common_word_2(x.split('\\t')[0],x.split('\\t')[2]))\n",
    "df_all['word_in_brand'] = df_all['product_info'].map(lambda x:str_common_word_2(x.split('\\t')[0],x.split('\\t')[3]))\n",
    "\n",
    "\n",
    "#these two features check exact whole str matches\n",
    "df_all['query_in_title'] = df_all['product_info'].map(lambda x:str_whole_word(x.split('\\t')[0],x.split('\\t')[1],0))\n",
    "df_all['query_in_description'] = df_all['product_info'].map(lambda x:str_whole_word(x.split('\\t')[0],x.split('\\t')[2],0))\n",
    "\n",
    "#how much of the brand is in the title/description\n",
    "#df_all['brand_in_query'] =  df_all['product_info'].map(lambda x:str_whole_word(x.split('\\t')[3],x.split('\\t')[0],0))\n",
    "#df_all['brand_in_title'] =  df_all['product_info'].map(lambda x:str_whole_word(x.split('\\t')[3],x.split('\\t')[1],0))\n",
    "#df_all['brand_in_description'] =  df_all['product_info'].map(lambda x:str_whole_word(x.split('\\t')[3],x.split('\\t')[2],0))\n",
    "\n",
    "\n",
    "#these two features (how much of the search query is in title && how much of the SQ is in description)\n",
    "df_all['ratio_title'] = df_all['word_in_title']/df_all['len_of_query']\n",
    "df_all['ratio_description'] = df_all['word_in_description']/df_all['len_of_query']\n",
    "df_all['ratio_brand'] = df_all['word_in_brand']/df_all['len_of_brand'] #ERROR (replace lob by loq)\n",
    "\n",
    "#ratio in terms of chars\n",
    "df_all['ratio_title_to_chars'] = df_all['len_of_title_in_chars'] / df_all['len_of_query_in_chars'] \n",
    "df_all['ratio_title_query_words'] = df_all['len_of_query']/df_all['len_of_title']\n",
    "\n",
    "#actual length of features\n",
    "df_all['actual_lenght_of_query'] = df_all['len_of_query'] / df_all['len_of_query_in_chars']\n",
    "df_all['actual_lenght_of_title'] = df_all['len_of_title'] / df_all['len_of_title_in_chars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#BRAND RELATED FEATURES\n",
    "\n",
    "#df_all['attr'] = df_all['search_term']+\"\\t\"+df_all['brand']\n",
    "#df_all['word_in_brand'] = df_all['attr'].map(lambda x:str_common_word_2(x.split('\\t')[0],x.split('\\t')[1]))\n",
    "#df_all['query_in_brand'] = df_all['attr'].map(lambda x:str_whole_word(x.split('\\t')[0],x.split('\\t')[1],0))\n",
    "#df_all['ratio_brand'] = df_all['word_in_brand']/df_all['len_of_brand']\n",
    "#df_all['ratio_brand_query_chars'] = df_all['len_of_query_in_chars']/df_all['len_of_brand_in_chars']\n",
    "df_brand = pd.unique(df_all.brand.ravel())\n",
    "d={}\n",
    "i = 1\n",
    "for s in df_brand:\n",
    "    d[s]=i\n",
    "    i+=1\n",
    "df_all['brand_feature'] = df_all['brand'].map(lambda x:d[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Basically get rids of all the content (text) features...Can be a drawback since we lost all the context feature. \n",
    "df_all = df_all.drop(['brand','search_term','product_title','product_description','product_info','clean_term','clean_title'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  product_uid  relevance  n_grams_clean_title_3  n_grams_stemmed_title_3  \\\n",
      "0   2       100001       3.00               0.142857                 0.083333   \n",
      "1   3       100001       2.50               0.000000                 0.083333   \n",
      "2   9       100002       3.00               0.238095                 0.250000   \n",
      "3  16       100005       2.33               0.238095                 0.208333   \n",
      "4  17       100005       2.67               0.380952                 0.375000   \n",
      "\n",
      "   n_grams_clean_title_4  n_grams_stemmed_title_4  n_grams_clean_title_5  \\\n",
      "0               0.095238                 0.041667               0.047619   \n",
      "1               0.000000                 0.000000               0.000000   \n",
      "2               0.142857                 0.041667               0.047619   \n",
      "3               0.142857                 0.166667               0.095238   \n",
      "4               0.333333                 0.333333               0.285714   \n",
      "\n",
      "   n_grams_stemmed_title_5  len_of_query  len_of_title  len_of_brand  \\\n",
      "0                 0.000000             2             4             2   \n",
      "1                 0.000000             2             4             2   \n",
      "2                 0.000000             2            11             4   \n",
      "3                 0.125000             3            12             1   \n",
      "4                 0.291667             3            12             1   \n",
      "\n",
      "   len_of_query_in_chars  len_of_title_in_chars  len_of_brand_in_chars  \\\n",
      "0                     12                     30                     17   \n",
      "1                      9                     30                     17   \n",
      "2                      9                     71                     26   \n",
      "3                     16                     76                      5   \n",
      "4                     18                     76                      5   \n",
      "\n",
      "   word_in_title  word_in_description  word_in_brand  query_in_title  \\\n",
      "0              1                    1              0               0   \n",
      "1              0                    0              0               0   \n",
      "2              1                    1              1               0   \n",
      "3              1                    1              0               0   \n",
      "4              3                    2              0               1   \n",
      "\n",
      "   query_in_description  ratio_title  ratio_description  ratio_brand  \\\n",
      "0                     0     0.500000           0.500000         0.00   \n",
      "1                     0     0.000000           0.000000         0.00   \n",
      "2                     0     0.500000           0.500000         0.25   \n",
      "3                     0     0.333333           0.333333         0.00   \n",
      "4                     0     1.000000           0.666667         0.00   \n",
      "\n",
      "   ratio_title_to_chars  ratio_title_query_words  actual_lenght_of_query  \\\n",
      "0              2.500000                 0.500000                0.166667   \n",
      "1              3.333333                 0.500000                0.222222   \n",
      "2              7.888889                 0.181818                0.222222   \n",
      "3              4.750000                 0.250000                0.187500   \n",
      "4              4.222222                 0.250000                0.166667   \n",
      "\n",
      "   actual_lenght_of_title  brand_feature  \n",
      "0                0.133333              1  \n",
      "1                0.133333              1  \n",
      "2                0.154930              2  \n",
      "3                0.157895              3  \n",
      "4                0.157895              3  \n"
     ]
    }
   ],
   "source": [
    "#pd.options.display.max_colwidth = 20\n",
    "pd.options.display.max_columns = 30\n",
    "print (df_all[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = df_all.iloc[:num_train]\n",
    "df_test = df_all.iloc[num_train:]\n",
    "id_test = df_test['id']\n",
    "y_train = df_train['relevance'].values\n",
    "#creates an array of only values (drops the headers and columns values provided)\n",
    "X_train = df_train.drop(['id','relevance'],axis=1).values\n",
    "X_test = df_test.drop(['id','relevance'],axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.459128846888\n"
     ]
    }
   ],
   "source": [
    "#using random regressor\n",
    "rf = RandomForestRegressor(n_estimators=15, max_depth=6, random_state=0, criterion='mse')\n",
    "clf = BaggingRegressor(rf, n_estimators=45, max_samples=0.1, random_state=25)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print (clf.score(X_train, y_train)**0.5)\n",
    "#RMSE = mean_squared_error(X_test,y_pred)**5\n",
    "#print (RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({\"id\": id_test, \"relevance\":y_pred}).to_csv('submission_v15.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#using the classifier from the rf_mean_square_error / code online\\nrfr = RandomForestRegressor(n_jobs = -1)\\nclf = pipeline.Pipeline([(\\'rfr\\', rfr)])\\nparam_grid = {\\'rfr__n_estimators\\':[400], \\'rfr__max_depth\\':[13]}\\n#param_grid = {}\\n#param_grid = {\\'rfr__n_estimators\\':list(range(34,50,1)), \\'rfr__max_depth\\':list(range(13,15,1))}\\nmodel = grid_search.GridSearchCV(estimator = clf, param_grid = param_grid, n_jobs = -1, cv = 10, verbose = 0, scoring=RMSE)\\nmodel.fit(X_train, y_train)\\n\\nprint(\"Best parameters found by grid search:\")\\nprint(model.best_params_)\\nprint(\"Best CV score:\")\\nprint(model.best_score_)\\n\\ny_pred = model.predict(X_test)\\nprint(len(y_pred))\\n'"
      ]
     },
     "execution_count": 879,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#using the classifier from the rf_mean_square_error / code online\n",
    "rfr = RandomForestRegressor(n_jobs = -1)\n",
    "clf = pipeline.Pipeline([('rfr', rfr)])\n",
    "param_grid = {'rfr__n_estimators':[400], 'rfr__max_depth':[13]}\n",
    "#param_grid = {}\n",
    "#param_grid = {'rfr__n_estimators':list(range(34,50,1)), 'rfr__max_depth':list(range(13,15,1))}\n",
    "model = grid_search.GridSearchCV(estimator = clf, param_grid = param_grid, n_jobs = -1, cv = 10, verbose = 0, scoring=RMSE)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found by grid search:\")\n",
    "print(model.best_params_)\n",
    "print(\"Best CV score:\")\n",
    "print(model.best_score_)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(len(y_pred))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
